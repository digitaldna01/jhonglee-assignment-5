{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "- Handle missing values and ensure that all relevant features are properly scaled.\n",
    "- Analyze categorical and numerical variables to determine which are most important for churn prediction.\n",
    "- Perform feature selection or engineering to improve model performance.\n",
    "\n",
    "# Model Building:\n",
    "- Implement K-Nearest Neighbors (KNN) from scratch. Pre-built libraries (e.g., scikit-learn, TensorFlow) for KNN are not allowed.\n",
    "- Split the dataset and train the model to classify customers into churn and non-churn categories.\n",
    "- Explore different values for K and choose the optimal one based on performance metrics.\n",
    "- Tune the model by adjusting hyperparameters such as the distance metric (Euclidean, Manhattan, etc.).\n",
    "\n",
    "# Model Evaluation:\n",
    "- Evaluate the model using metrics such as accuracy, precision, recall, F1-score, area under the ROC curve.\n",
    "- Use cross-validation to ensure the model is not overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "    \n",
    "    def euclidean_distance(self, point1, point2):\n",
    "        euclidean_distance = np.linalg.norm(point1 - point2)\n",
    "        return euclidean_distance\n",
    "    \n",
    "    def manhattan_distance(self, point1, point2):\n",
    "        manhattan_distance = np.sum(np.abs(point1 - point2))\n",
    "        return manhattan_distance\n",
    "\n",
    "    def cosine_distance(self, point1, point2):\n",
    "        cosine_distance = 1 - np.dot(point1, point2) / (np.linalg.norm(point1) * np.linalg.norm(point2))\n",
    "        return cosine_distance\n",
    "\n",
    "    def chebyshev_distance(self, point1, point2):\n",
    "        chebyshev_distance = np.max(np.abs(point1 - point2))\n",
    "        return chebyshev_distance\n",
    "\n",
    "    # Use Training data to store the data in KNN Model\n",
    "    def fit(self, X, y):\n",
    "        # TODO: Implement the fit method\n",
    "        self.train_x = X\n",
    "        self.train_y = y\n",
    "        if (len(self.train_x) != len(self.train_y)):\n",
    "            raise ValueError(f\"Train X and Train Y size differ, Train size: {len(self.train_x)} Test size: {len(self.train_y)}\")\n",
    "        self.train_size = len(self.train_x)\n",
    "    \n",
    "    # Depends on the input distance Metric, calculate the distance based on the input\n",
    "    def compute_distance(self, X1, X2):\n",
    "        # TODO: Implement distance computation based on self.distance_metric\n",
    "        # Hint: Use numpy operations for efficient computation\n",
    "        if (self.distance_metric == \"euclidean\"):\n",
    "            return self.euclidean_distance(X1, X2)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return self.manhattan_distance(X1, X2)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            return self.cosine_distance(X1, X2)\n",
    "        elif self.distance_metric == 'chebyshev':\n",
    "            return self.chebyshev_distance(X1, X2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "        \n",
    "    def n_closest_to(self, X):\n",
    "        # Array to save k closest distances\n",
    "        distances = []\n",
    "        \n",
    "        # iterate the number of train values\n",
    "        for i in range(self.train_size):\n",
    "            distance = self.compute_distance(self.train_x[i], X)\n",
    "            distances.append(distance)\n",
    "        \n",
    "        closest_points = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # [distances[i] for i in closest_points]\n",
    "        return closest_points \n",
    "    \n",
    "    def majority(self, closest_points):\n",
    "        class_counts = {}\n",
    "        \n",
    "        for i in closest_points:\n",
    "            label = self.train_y[i]\n",
    "            if label in class_counts:\n",
    "                class_counts[label]+= 1\n",
    "            else:\n",
    "                class_counts[label] = 1\n",
    "        \n",
    "        majority_class = max(class_counts, key=class_counts.get)\n",
    "            \n",
    "        return majority_class\n",
    "\n",
    "    # Predict the Class of Test_X data \n",
    "    def predict(self, X_test):\n",
    "        # TODO: Implement the predict method\n",
    "        # X_test = X_test.to_numpy()\n",
    "        predictions = []\n",
    "        \n",
    "        for index in  range(len(X_test)):\n",
    "            closest_points = self.n_closest_to(X_test[index])\n",
    "            predicted_class = self.majority(closest_points)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        # Simplified probability prediction method (for ROC AUC)\n",
    "        # X_test = X_test.to_numpy()\n",
    "        probabilities = []\n",
    "        \n",
    "        for index in range(len(X_test)): \n",
    "            closest_points = self.n_closest_to(X_test[index])\n",
    "            closest_labels = self.train_y[closest_points]\n",
    "            pos_prob = np.sum(closest_labels) / self.k  # Probability of positive class\n",
    "            probabilities.append(pos_prob)\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually implementing one-hot encoding\n",
    "def manual_one_hot_encoding(df, column):\n",
    "    unique_values = df[column].unique()  # Find unique values in the column\n",
    "    \n",
    "    for unique_value in unique_values:\n",
    "        new_column_name = f\"{column}_{unique_value}\"\n",
    "        df[new_column_name] = df[column].apply(lambda x: 1 if x == unique_value else 0)\n",
    "    \n",
    "    df = df.drop(column, axis=1)  # Drop the original column\n",
    "    return df\n",
    "\n",
    "# Function to apply manual feature scaling\n",
    "def manual_standard_scaler(df, column):\n",
    "    mean = np.mean(df[column])\n",
    "    std = np.std(df[column])\n",
    "    \n",
    "    # Apply the standardization formula\n",
    "    df[column] = (df[column] - mean) / std\n",
    "    return df\n",
    "\n",
    "# Function to perform SVD and reduce dimensionality\n",
    "def svd_reduction(X_train, X_test, n_components):\n",
    "    # Step 1: Center the data\n",
    "    X_train_centered = X_train - np.mean(X_train, axis=0)\n",
    "\n",
    "    # Step 2: Compute SVD on the training set\n",
    "    U, S, Vt = np.linalg.svd(X_train_centered)\n",
    "\n",
    "    # Step 3: Select the top n_components\n",
    "    U_reduced = U[:, :n_components]\n",
    "    S_reduced = np.diag(S[:n_components])\n",
    "\n",
    "    # Step 4: Reconstruct the training data using the reduced components\n",
    "    X_train_reduced = np.dot(U_reduced, S_reduced)\n",
    "\n",
    "    # Step 5: Center the test data using the training mean\n",
    "    X_test_centered = X_test - np.mean(X_train, axis=0)\n",
    "    # Step 6: Project the test data onto the same components\n",
    "    X_test_reduced = np.dot(X_test_centered, Vt[:n_components, :].T)\n",
    "\n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "# Define data preprocessing function\n",
    "def preprocess_data_svd(train_path, test_path, n_components=2):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    X_test = pd.read_csv(test_path)\n",
    "    \n",
    "    y = train_data['Exited']\n",
    "    X = train_data.drop(['Exited'], axis=1)\n",
    "    \n",
    "    X = X.drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "    X_test = X_test.drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "    \n",
    "    X = manual_one_hot_encoding(X, 'Geography')\n",
    "    X = manual_one_hot_encoding(X, 'Gender')\n",
    "    \n",
    "    X_test = manual_one_hot_encoding(X_test, 'Geography')\n",
    "    X_test = manual_one_hot_encoding(X_test, 'Gender')\n",
    "    \n",
    "    # Apply standardization to specific columns\n",
    "    for column in ['CreditScore', 'Age', 'Balance']:\n",
    "        X = manual_standard_scaler(X, column)\n",
    "        X_test = manual_standard_scaler(X_test, column)\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    X = X.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    \n",
    "    # Apply SVD for dimensionality reduction\n",
    "    X_reduced, X_test_reduced = svd_reduction(X, X_test, n_components=n_components)\n",
    "\n",
    "    # Handle categorical variables, scale features, etc.\n",
    "    return X_reduced, y.to_numpy(), X_test_reduced\n",
    "# Define data preprocessing function\n",
    "\n",
    "def preprocess_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    X_test = pd.read_csv(test_path)\n",
    "    \n",
    "    y = train_data['Exited']\n",
    "    X = train_data.drop(['Exited'], axis=1)\n",
    "    \n",
    "    X = X.drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "    X_test = X_test.drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "    \n",
    "    X = manual_one_hot_encoding(X, 'Geography')\n",
    "    X = manual_one_hot_encoding(X, 'Gender')\n",
    "    \n",
    "    X_test = manual_one_hot_encoding(X_test, 'Geography')\n",
    "    X_test = manual_one_hot_encoding(X_test, 'Gender')\n",
    "    \n",
    "    # Apply standardization to specific columns\n",
    "    for column in ['CreditScore', 'Age', 'Balance']:\n",
    "        X = manual_standard_scaler(X, column)\n",
    "        X_test = manual_standard_scaler(X_test, column)\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    X = X.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    \n",
    "    # Apply SVD for dimensionality reduction\n",
    "\n",
    "    # Handle categorical variables, scale features, etc.\n",
    "    return X, y.to_numpy(), X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    true_negative = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    false_negative = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    accuracy = (true_positive + true_negative) / len(y_true)\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def compute_roc_auc(y_true, y_scores):\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    \n",
    "    # TP, FP, TN, FN 계산\n",
    "    for threshold in thresholds:\n",
    "        y_pred =  (y_scores >= threshold).astype(int)\n",
    "        true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        true_negative = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        false_positive = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        false_negative = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        # TPR (True Positive Rate)\n",
    "        tpr_value = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0  # TPR 정의\n",
    "        tpr.append(tpr_value)\n",
    "\n",
    "        # FPR (False Positive Rate)\n",
    "        fpr_value = false_positive / (false_positive + true_negative) if (false_positive + true_negative) > 0 else 0  # FPR 정의\n",
    "        fpr.append(fpr_value)\n",
    "\n",
    "\n",
    "    # ROC 곡선 아래 면적 계산 (AUC)\n",
    "    tpr = np.array(tpr)\n",
    "    fpr = np.array(fpr)\n",
    "    \n",
    "    # Make sure we have at least two points for AUC calculation\n",
    "    if len(tpr) < 2 or len(fpr) < 2:\n",
    "        raise ValueError(\"Not enough points to compute AUC.\")\n",
    "    \n",
    "    roc_auc = np.trapz(tpr, fpr)  # Trapezoidal rule for AUC\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "# Define cross-validation function\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # TODO: Implement cross-validation\n",
    "    # Compute ROC AUC scores\n",
    "    fold_size = len(X) // n_splits\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    roc_auc_list = []\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        # Split the data\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size if i < n_splits - 1 else len(X)\n",
    "        \n",
    "        X_train = np.concatenate([X[:start], X[end:]])\n",
    "        y_train = np.concatenate([y[:start], y[end:]])\n",
    "        X_test = X[start:end]\n",
    "        y_test = y[start:end]\n",
    "        \n",
    "        # Fit the KNN model\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = knn.predict(X_test)\n",
    "        y_scores = knn.predict_proba(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test, y_pred)\n",
    "        roc_auc = compute_roc_auc(y_test, y_scores)\n",
    "\n",
    "        # Append the metrics to the lists\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "    # Calculate the mean of each metric\n",
    "    mean_accuracy = np.mean(accuracy_list)\n",
    "    mean_precision = np.mean(precision_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "    mean_f1 = np.mean(f1_list)\n",
    "    mean_roc_auc = np.mean(roc_auc_list)\n",
    "\n",
    "    # Return the average metrics\n",
    "    return {\n",
    "        'accuracy': mean_accuracy,\n",
    "        'precision': mean_precision,\n",
    "        'recall': mean_recall,\n",
    "        'f1_score': mean_f1,\n",
    "        'roc_auc': mean_roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction :  [0 1 1 0 1] \n",
      "\n",
      "probabilities : [0. 1. 1. 0. 1.]\n",
      "AUC: -1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = np.array([[0.1, 0.1],\n",
    "                    [0.4, 0.4],\n",
    "                    [0.5, 0.5],\n",
    "                    [0.9, 0.8],\n",
    "                    [0.8, 0.6]])\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.array([[0.1, 0.1],  # Close to first training point\n",
    "                   [0.4, 0.4],  # Close to second training point\n",
    "                   [0.5, 0.5],  # Close to third training point\n",
    "                   [0.9, 0.9],  # Close to fourth training point\n",
    "                   [0.8, 0.7]]) # Close to fifth training point\n",
    "\n",
    "y_true = np.array([0, 1, 1, 0, 1])  # 실제 레이블\n",
    "\n",
    "knn = KNN(k=1, distance_metric='euclidean')\n",
    "\n",
    "knn.fit(X_train, y_true)\n",
    "prediction = knn.predict(X_test)\n",
    "probabilities = knn.predict_proba(X_test)\n",
    "print(\"prediction : \", prediction, \"\\n\")\n",
    "print(\"probabilities :\", probabilities)\n",
    "y_scores = [0.1, 0.4, 0.35, 0.8, 0.7]  # 예측 확률\n",
    "y_true = np.array(y_true)\n",
    "y_scores = np.array(y_scores)\n",
    "# AUC 계산\n",
    "auc_score = compute_roc_auc(y_true, probabilities)\n",
    "# auc_score = roc_auc_score(y_true, y_scores, 0.5)\n",
    "\n",
    "print(f\"AUC: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: {'accuracy': 0.7664, 'precision': 0.2385216849400313, 'recall': 0.06956743267647551, 'f1_score': 0.10735214393219628, 'roc_auc': -0.5096462263694129}\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('cs-506-predicting-customer-churn-using-knn/train.csv', 'cs-506-predicting-customer-churn-using-knn/test.csv', 2)\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=5, distance_metric='manhattan')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: {'accuracy': 0.7716000000000001, 'precision': 0.2668292377308332, 'recall': 0.0725209283940105, 'f1_score': 0.11375525816834915, 'roc_auc': -0.5335686112592383}\n",
      "k: 3, distance_metric: euclidean, Score: {'accuracy': 0.7558666666666667, 'precision': 0.28402735013687613, 'recall': 0.13439466270438308, 'f1_score': 0.18205683315625296, 'roc_auc': -0.5442510683978025}\n",
      "\n",
      "k: 3, distance_metric: manhattan, Score: {'accuracy': 0.7641333333333333, 'precision': 0.3159710056110793, 'recall': 0.14201601155555837, 'f1_score': 0.19571620765256964, 'roc_auc': -0.5759630274095184}\n",
      "\n",
      "k: 3, distance_metric: chebyshev, Score: {'accuracy': 0.7461333333333333, 'precision': 0.24686502150063233, 'recall': 0.1231219750833908, 'f1_score': 0.1637603948192148, 'roc_auc': -0.5218395882741795}\n",
      "\n",
      "k: 5, distance_metric: euclidean, Score: {'accuracy': 0.7716000000000001, 'precision': 0.2668292377308332, 'recall': 0.0725209283940105, 'f1_score': 0.11375525816834915, 'roc_auc': -0.5335686112592383}\n",
      "\n",
      "k: 5, distance_metric: manhattan, Score: {'accuracy': 0.7776666666666666, 'precision': 0.3139266394349004, 'recall': 0.08179003823869999, 'f1_score': 0.12957165853385053, 'roc_auc': -0.5683638367502002}\n",
      "\n",
      "k: 5, distance_metric: chebyshev, Score: {'accuracy': 0.7672, 'precision': 0.2419923762032346, 'recall': 0.0701820481603429, 'f1_score': 0.10841908243037417, 'roc_auc': -0.515598400166377}\n",
      "\n",
      "k: 10, distance_metric: euclidean, Score: {'accuracy': 0.7896666666666666, 'precision': 0.300227754107368, 'recall': 0.029643818021698985, 'f1_score': 0.05390674034433991, 'roc_auc': -0.5225209387375751}\n",
      "\n",
      "k: 10, distance_metric: manhattan, Score: {'accuracy': 0.7934666666666665, 'precision': 0.3920789876234182, 'recall': 0.03431560393025244, 'f1_score': 0.06293605820914362, 'roc_auc': -0.5550105600424503}\n",
      "\n",
      "k: 10, distance_metric: chebyshev, Score: {'accuracy': 0.7882666666666667, 'precision': 0.2778416613571757, 'recall': 0.02833756613405175, 'f1_score': 0.05131209380219535, 'roc_auc': -0.5090372063467575}\n",
      "\n",
      "k: 15, distance_metric: euclidean, Score: {'accuracy': 0.7951333333333334, 'precision': 0.245793489809717, 'recall': 0.005308378989874597, 'f1_score': 0.010324192395163943, 'roc_auc': -0.5150099232715066}\n",
      "\n",
      "k: 15, distance_metric: manhattan, Score: {'accuracy': 0.7956000000000001, 'precision': 0.27612010796221326, 'recall': 0.0059527880257092165, 'f1_score': 0.011585921897199262, 'roc_auc': -0.540529399028929}\n",
      "\n",
      "k: 15, distance_metric: chebyshev, Score: {'accuracy': 0.7943333333333333, 'precision': 0.16904761904761903, 'recall': 0.004679853661125823, 'f1_score': 0.009009183449483397, 'roc_auc': -0.505384334272492}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('cs-506-predicting-customer-churn-using-knn/train.csv', 'cs-506-predicting-customer-churn-using-knn/test.csv')\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=5, distance_metric='euclidean')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "def compute_model_score(X, y, k, distance_metric):\n",
    "    # KNN 모델 생성 및 평가\n",
    "    knn = KNN(k=k, distance_metric=distance_metric)\n",
    "    scores = cross_validate(X, y, knn)  # 사용자 정의 cross-validation 함수\n",
    "    return scores # 평균 점수 반환\n",
    "\n",
    "# TODO: hyperparamters tuning\n",
    "k_values = [3, 5, 10, 15]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for k in k_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        score = compute_model_score(X, y, k, distance_metric)\n",
    "        print(f\"k: {k}, distance_metric: {distance_metric}, Score: {score}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "# knn = ...\n",
    "# knn.fit(X, y)\n",
    "# test_predictions = knn.predict(X_test)\n",
    "\n",
    "# # Save test predictions\n",
    "# pd.DataFrame({'id': pd.read_csv('/path/of/test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 15, N_components: 2, Score: {'accuracy': 0.7943333333333334, 'precision': 0.18596638655462186, 'recall': 0.004621540761361795, 'f1_score': 0.008979599302460535, 'roc_auc': -0.5045293925101082}\n",
      "\n",
      "k: 15, N_components: 5, Score: {'accuracy': 0.7952, 'precision': 0.26412698412698415, 'recall': 0.0062774633503845404, 'f1_score': 0.0122109750615764, 'roc_auc': -0.5265599206468567}\n",
      "\n",
      "k: 15, N_components: 8, Score: {'accuracy': 0.7952666666666667, 'precision': 0.253015873015873, 'recall': 0.0059527880257092165, 'f1_score': 0.011577089711257926, 'roc_auc': -0.5306467000315396}\n",
      "\n",
      "k: 20, N_components: 2, Score: {'accuracy': 0.7968666666666666, 'precision': 0.35189393939393937, 'recall': 0.004331691849373528, 'f1_score': 0.008503208085010436, 'roc_auc': -0.5077867063411559}\n",
      "\n",
      "k: 20, N_components: 5, Score: {'accuracy': 0.7970666666666666, 'precision': 0.5241666666666667, 'recall': 0.003634613781057331, 'f1_score': 0.007166918929606902, 'roc_auc': -0.5261602213103341}\n",
      "\n",
      "k: 20, N_components: 8, Score: {'accuracy': 0.7973999999999999, 'precision': 0.5416666666666666, 'recall': 0.004325241346234283, 'f1_score': 0.008521183818164102, 'roc_auc': -0.5309770989535405}\n",
      "\n",
      "k: 25, N_components: 2, Score: {'accuracy': 0.7975333333333333, 'precision': 0.1, 'recall': 0.0003322259136212625, 'f1_score': 0.0006622516556291392, 'roc_auc': -0.5130668121684534}\n",
      "\n",
      "k: 25, N_components: 5, Score: {'accuracy': 0.7978, 'precision': 0.2, 'recall': 0.0006633517414358321, 'f1_score': 0.0013223176622297994, 'roc_auc': -0.5298824412641736}\n",
      "\n",
      "k: 25, N_components: 8, Score: {'accuracy': 0.7976666666666666, 'precision': 0.16666666666666666, 'recall': 0.0006633517414358321, 'f1_score': 0.0013212302388251854, 'roc_auc': -0.5313620043616258}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "def compute_model_score(X, y, k, n_component):\n",
    "    X, y, X_test = preprocess_data('cs-506-predicting-customer-churn-using-knn/train.csv', 'cs-506-predicting-customer-churn-using-knn/test.csv', n_component)\n",
    "    # KNN 모델 생성 및 평가\n",
    "    knn = KNN(k=k, distance_metric='manhattan')\n",
    "    scores = cross_validate(X, y, knn)  # 사용자 정의 cross-validation 함수\n",
    "    return scores # 평균 점수 반환\n",
    "\n",
    "# hyperparamters tuning\n",
    "k_values = [15, 20, 25]\n",
    "n_values = [2, 5, 8]\n",
    "\n",
    "\n",
    "for k in k_values:\n",
    "    for n in n_values:\n",
    "        score = compute_model_score(X, y, k, n)\n",
    "        print(f\"k: {k}, N_components: {n}, Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('cs-506-predicting-customer-churn-using-knn/train.csv', 'cs-506-predicting-customer-churn-using-knn/test.csv')\n",
    "\n",
    "knn = KNN(k=20, distance_metric='manhattan')\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# # Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('cs-506-predicting-customer-churn-using-knn/test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
